14 L.-C Chen, Y. Zhu, G. Papandreou, F. Schroﬀ, and H. Adam
Fig.6.Visualization results on valset. The last row shows a failure mode.
Backbone Decoder ASPP Image-Level mIOU
X-65 /check /check 77.33
X-65 /check /check /check 78.79
X-65 /check /check 79.14
X-71 /check /check 79.55Method Coarse mIOU
ResNet-38 [ 83]/check 80.6
PSPNet [ 24]/check 81.2
Mapillary [ 86]/check 82.0
DeepLabv3 /check 81.3
DeepLabv3+ /check 82.1
(a)valset results (b) testset results
Table 7. (a) DeepLabv3+ on the Cityscapes valset when trained with trainﬁneset.
(b) DeepLabv3+ on Cityscapes testset.Coarse: Usetrainextraset (coarse annota-
tions) as well. Only a few top models are listed in this table.
models.AsshowninTab. 7(b),ourproposedDeepLabv3+attainsaperformance
of 82.1% on the test set, setting a new state-of-art performance on Citysc apes.
5 Conclusion
Ourproposedmodel“DeepLabv3+”employstheencoder-decoderstru cturewhere
DeepLabv3 is used to encode the rich contextual information and a simp le yet
eﬀective decoder module is adopted to recover the object boundari es. One could
also apply the atrous convolution to extract the encoder features at an arb itrary
resolution, depending on the available computation resources. We also explore
the Xception model and atrous separable convolution to make the proposed
model faster and stronger. Finally, our experimental results show t hat the pro-
posed model sets a new state-of-the-art performance on PASCAL VOC 2012 and
Cityscapes datasets.
Acknowledgments We would like to acknowledge the valuable discussions
with Haozhi Qi and Jifeng Dai about Aligned Xception, the feedback from Chen
Sun, and the support from Google Mobile Vision team.